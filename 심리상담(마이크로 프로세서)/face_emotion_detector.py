import cv2
import numpy as np
import tensorflow.lite as tflite
import glob

# ==============================
# ğŸ¯ TensorFlow Lite ëª¨ë¸ ë¡œë“œ
# ==============================
interpreter = tflite.Interpreter(model_path="best_model.tflite")
interpreter.allocate_tensors()

input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# ê°ì • ë ˆì´ë¸” (ëª¨ë¸ í•™ìŠµ ìˆœì„œì— ë§ê²Œ ì¡°ì •)
emotion_labels = ['í–‰ë³µ', 'ìŠ¬í””', 'í™”ë‚¨', 'ë†€ëŒ']


def find_camera_device():
    """USB ë˜ëŠ” CSI ì¹´ë©”ë¼ ìë™ íƒìƒ‰"""
    for i in range(5):
        cap = cv2.VideoCapture(i)
        if cap.isOpened():
            cap.release()
            return i
    print("âŒ ì¹´ë©”ë¼ ì¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    return None


def predict_emotion(face_img):
    """ì…ë ¥ëœ ì–¼êµ´ ì´ë¯¸ì§€(48x48)ë¡œ ê°ì • ì˜ˆì¸¡"""
    img = face_img.astype("float32") / 255.0
    img = np.expand_dims(img, axis=0)
    img = np.expand_dims(img, axis=-1)

    interpreter.set_tensor(input_details[0]['index'], img)
    interpreter.invoke()
    preds = interpreter.get_tensor(output_details[0]['index'])
    return emotion_labels[int(np.argmax(preds))]


def get_emotion_from_face():
    """ì¹´ë©”ë¼ì—ì„œ ì–¼êµ´ì„ ì¸ì‹í•˜ê³  ê°ì •ì„ ì¶”ì¶œ"""
    cam_index = find_camera_device()
    if cam_index is None:
        return None

    cap = cv2.VideoCapture(cam_index)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

    print("ğŸ¥ ë¼ì¦ˆë² ë¦¬íŒŒì´ ì¹´ë©”ë¼ ê°ì • ì¸ì‹ ì¤‘... (ESC ëˆŒëŸ¬ ì¢…ë£Œ)")

    detected_emotion = None

    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.3, 5)

        for (x, y, w, h) in faces:
            face = gray[y:y+h, x:x+w]
            face = cv2.resize(face, (48, 48))
            emotion = predict_emotion(face)
            detected_emotion = emotion

            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)
            cv2.putText(frame, emotion, (x, y-10),
                        cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)

        cv2.imshow("Emotion Detection (Raspberry Pi)", frame)

        # ESC í‚¤(27)ë¡œ ì¢…ë£Œ
        if cv2.waitKey(5) & 0xFF == 27:
            break

    cap.release()
    cv2.destroyAllWindows()

    if detected_emotion:
        print(f"âœ… ê°ì • ì¸ì‹ ì™„ë£Œ: {detected_emotion}")
        return detected_emotion
    else:
        print("ğŸ˜¢ ê°ì •ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        return None
